{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net cell segmentation on the DIC-C2DH-HeLa dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import gc, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet.utils import UNetHelper\n",
    "from unet.losses import IoU, dice_loss, unet_sample_weights\n",
    "from unet.augmentation import elastic_deformation, grid_deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "tf_dir = \"TFData\"\n",
    "batch_size = 8\n",
    "max_epochs = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seed\n",
    "\n",
    "For resetting the seed when running the training loop multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed = lambda seed=42: tf.keras.utils.set_random_seed(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training strategy\n",
    "\n",
    "This selection is based off the tools I have at my disposal: either 1 GPU at work or 2 on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = len(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "if gpus <= 1: \n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/GPU:0\")\n",
    "else: \n",
    "    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\n",
    "\n",
    "n_devices = strategy.num_replicas_in_sync\n",
    "print(f\"Using {n_devices} device(s).\")\n",
    "print(f\"Using {strategy.__class__.__name__}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img, mask):\n",
    "    \"\"\"\n",
    "    Contrast Limited Adaptive Histogram Equalization (CLAHE) step, \n",
    "    followed by sample weight calculation [0.0, 1.0] normalization. \n",
    "    CLAHE uses the default OpenCV parameters.\n",
    "    \"\"\"\n",
    "    clh = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clh_img = clh.apply(np.squeeze(img.numpy())) \n",
    "    sample_weights = unet_sample_weights(mask.numpy(), data_type=np.float32)\n",
    "    return (tf.constant(np.expand_dims(clh_img / 255.0, -1), dtype=tf.float32, shape=img.get_shape()), \n",
    "            mask,\n",
    "            tf.constant(sample_weights, dtype=tf.float32, shape=mask.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    minimum, maximum = arr.min(), arr.max()\n",
    "    return (arr - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (512, 512, 1)\n",
    "mask_shape = (512, 512)\n",
    "\n",
    "hela_train = tfds.load(\"hela_train\", data_dir=tf_dir)\n",
    "\n",
    "# Cache segment 01\n",
    "hela_train[\"01\"] = hela_train[\"01\"].map(lambda sample: tf.py_function(process_img, inp=[sample['image'], sample['mask']], \n",
    "                                                                      Tout=[tf.float32, tf.int32, tf.float32]),  \n",
    "                                        num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                   .map(lambda X, y, sw: (tf.ensure_shape(X, img_shape), \n",
    "                                                          tf.ensure_shape(y, mask_shape), \n",
    "                                                          tf.ensure_shape(sw, mask_shape)))\\\n",
    "                                   .cache(f\"{tf_dir}/TFCache/01_CLAHE_NORM\")\n",
    "example = list(hela_train[\"01\"].take(2))\n",
    "# Cache segment 02\n",
    "hela_train[\"02\"] = hela_train[\"02\"].map(lambda pair: tf.py_function(process_img, inp=[pair['image'], pair['mask']], \n",
    "                                                                    Tout=[tf.float32, tf.int32, tf.float32]),  \n",
    "                                        num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                   .map(lambda X, y, sw: (tf.ensure_shape(X, img_shape), \n",
    "                                                          tf.ensure_shape(y, mask_shape), \n",
    "                                                          tf.ensure_shape(sw, mask_shape)))\\\n",
    "                                   .cache(f\"{tf_dir}/TFCache/02_CLAHE_NORM\")\n",
    "example += list(hela_train[\"02\"].take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(example), 3, figsize=(10, 5 * len(example)))\n",
    "\n",
    "axes[0,0].set_title(\"Images\")\n",
    "axes[0,1].set_title(\"Masks\")\n",
    "axes[0,2].set_title(\"Sample weights\")\n",
    "\n",
    "for row, ex in zip(axes, example):\n",
    "    for ax, img in zip(row, ex):\n",
    "        ax.imshow(min_max(img), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "fig.tight_layout(h_pad=-15.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pipeline(X, y, w):\n",
    "    # Add channel axis\n",
    "    y = tf.expand_dims(y, axis=-1)\n",
    "    w = tf.expand_dims(w, axis=-1)\n",
    "    # Horizontal flip\n",
    "    if tf.random.uniform((), 0.0, 1.0) >= 0.5:\n",
    "        X = tf.image.flip_left_right(X)\n",
    "        y = tf.image.flip_left_right(y)\n",
    "        w = tf.image.flip_left_right(w)\n",
    "    # Vertical flip\n",
    "    if tf.random.uniform((), 0.0, 1.0) >= 0.5:\n",
    "        X = tf.image.flip_up_down(X)\n",
    "        y = tf.image.flip_up_down(y)\n",
    "        w = tf.image.flip_up_down(w)\n",
    "    # Grid deformation\n",
    "    if tf.random.uniform((), 0.0, 1.0) >= 0.5:\n",
    "        grid_size = 5\n",
    "        distort_limits = (-0.35, 0.35)\n",
    "        X = grid_deformation(X, distort_limits=distort_limits, grid_size=grid_size, order=1)\n",
    "        y = grid_deformation(y, distort_limits=distort_limits, grid_size=grid_size, order=0)\n",
    "        w = grid_deformation(w, distort_limits=distort_limits, grid_size=grid_size, order=0)\n",
    "    # Elastic deformation\n",
    "    if tf.random.uniform((), 0.0, 1.0) >= 0.5:\n",
    "        alpha = 100.0\n",
    "        sigma = 5.0\n",
    "        auto_kSize = True\n",
    "        X = elastic_deformation(X, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=1)\n",
    "        y = elastic_deformation(y, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=0)\n",
    "        w = elastic_deformation(w, alpha=alpha, sigma=sigma, auto_kSize=auto_kSize, order=0)\n",
    "    return [X, tf.squeeze(y), tf.squeeze(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(example), 2, figsize=(8, 4 * len(example)))\n",
    "\n",
    "axes[0,0].set_title(\"Original\")\n",
    "axes[0,1].set_title(\"Augmented\")\n",
    "\n",
    "for row, (tmp_X, tmp_y, tmp_w) in zip(axes, example):\n",
    "    row[0].imshow(min_max(ex[0]), cmap=\"gray\")\n",
    "    row[0].axis(\"off\")\n",
    "    row[1].imshow(min_max(pipeline(tf.expand_dims(tmp_X, 0),\n",
    "                                   tf.expand_dims(tmp_y, 0),\n",
    "                                   tf.expand_dims(tmp_w, 0))[0][0]), cmap=\"gray\")\n",
    "    row[1].axis(\"off\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(helper, train_dataset, val_dataset=None, examples=None, epochs=100, ckpt_every=10, plot_every=1, verbose=True): # A helper function I wrote in a hurry.\n",
    "    history = []\n",
    "    ds_card = train_dataset.cardinality\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'\\nEpoch {epoch}/{epochs}')\n",
    "        # Learning rate schedule\n",
    "        if helper.opt_schedule is not None: \n",
    "            helper.optimizer.learning_rate = helper.opt_schedule(epoch)\n",
    "        # Create progress bar\n",
    "        if verbose: \n",
    "            progbar = tf.keras.utils.Progbar(target=ds_card)\n",
    "        # Run the training steps\n",
    "        for i, batch in enumerate(train_dataset):\n",
    "            loss, acc = helper.dist_train_step(batch)\n",
    "            # Update prog bar\n",
    "            if verbose:\n",
    "                progbar.update(i + 1, zip(['loss', 'acc'], [loss, acc]), finalize=False)\n",
    "        # Run for the validation set\n",
    "        if val_dataset is not None:\n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            for j, batch in enumerate(val_dataset):\n",
    "                vloss, vacc = helper.dist_val_step(batch)\n",
    "                val_loss += vloss\n",
    "                val_acc += vacc\n",
    "            val_loss /= (j + 1)\n",
    "            val_acc /= (j + 1)\n",
    "            history.append([loss, acc, val_loss, val_acc])\n",
    "            if verbose:\n",
    "                progbar.update(i, zip(['loss', 'acc', 'val_loss', 'val_acc', 'lr'], \n",
    "                                      [loss, acc, val_loss, val_acc, helper.optimizer.learning_rate.numpy()]), finalize=True)\n",
    "        else: \n",
    "            history.append([loss, acc])\n",
    "            if verbose:\n",
    "                progbar.update(i, zip(['loss', 'acc', 'lr'], [loss, acc, helper.optimizer.learning_rate.numpy()]), finalize=True)\n",
    "        # Save training checkpoint\n",
    "        if type(ckpt_every) is int: \n",
    "            if epoch % ckpt_every == 0: \n",
    "                helper.checkpoint.save(helper.checkpoint_dir)\n",
    "        # Plot training progression with the selected examples\n",
    "        if type(plot_every) is int: \n",
    "            if epoch % plot_every == 0 and examples is not None:\n",
    "                plt.close()\n",
    "                X, y = list(examples.take(1))[0]\n",
    "                image_list = [X.numpy()[0], y.numpy()[0], helper.model(X).numpy().argmax(axis=-1)[0]]\n",
    "                image_list = [(255.0 * img).astype('uint8') if img.dtype !='uint8' else img for img in image_list]\n",
    "                fig, ax = plt.subplots(1, 3, figsize=(14, 28))\n",
    "                ax[0].set_title(\"Image\")\n",
    "                ax[1].set_title(\"Mask\")\n",
    "                ax[2].set_title(\"Predicted Mask\")\n",
    "                for k in range(3): \n",
    "                    ax[k].imshow(image_list[k], cmap=\"gray\")\n",
    "                    ax[k].axis(\"off\")    \n",
    "                plt.show()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Nothing too fancy: GroupKFold with each of the recordings as a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_lr = 1.E-3\n",
    "\n",
    "lr_decay_start, lr_decay_rate, lr_decay_step = (2, 0.1, 3)\n",
    "\n",
    "model_param = {\"input_shape\": img_shape,\n",
    "               \"dropout\": 0.2}\n",
    "\n",
    "oof_dice = []\n",
    "oof_IoU = []\n",
    "\n",
    "fold = [[\"01\", \"02\"], [\"02\", \"01\"]]\n",
    "\n",
    "for i in range(2):\n",
    "    # In case we're running this cell over and over again when searching hyperparameters\n",
    "    try:\n",
    "        del helper\n",
    "    except:\n",
    "        pass\n",
    "    # Restore the random seed and clear the current TF graph\n",
    "    reset_seed()\n",
    "    K.clear_session()\n",
    "    # Set the augmentation, batching and distribution of the dataset.\n",
    "    # The augmentation .map() should come after both the .batch() and .cache()\n",
    "    # for increased variety of augmented samples.\n",
    "    training_size = hela_train[fold[i][0]].cardinality().numpy()\n",
    "    train_ds = hela_train[fold[i][0]].shuffle(training_size, reshuffle_each_iteration=True)\\\n",
    "                                     .repeat(np.lcm(batch_size, training_size) // (training_size))\\\n",
    "                                     .batch(batch_size, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                     .map(pipeline, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                     .prefetch(tf.data.AUTOTUNE)\n",
    "    dist_train = strategy.experimental_distribute_dataset(train_ds)\n",
    "    # Same thing for the validation split\n",
    "    validation_size = hela_train[fold[i][1]].cardinality().numpy()\n",
    "    val_ds = hela_train[fold[i][1]].map(lambda X, y, sw: (X, y))\\\n",
    "                                   .cache()\\\n",
    "                                   .batch(2 * batch_size, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dist_val = strategy.experimental_distribute_dataset(val_ds)\n",
    "    # GPU training\n",
    "    gc.collect()\n",
    "    with strategy.scope():    \n",
    "        gc.collect()\n",
    "        helper = UNetHelper(strategy=strategy,\n",
    "                            model_param=model_param,\n",
    "                            loss_func=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                            optimizer=tf.keras.optimizers.SGD(learning_rate=max_lr, momentum=0.99),\n",
    "                            #opt_schedule=tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[5,], values=[1e-2, 1e-3]),\n",
    "                            )\n",
    "        if train_model:\n",
    "            train(helper, dist_train, dist_val, val_ds.rebatch(1), max_epochs, ckpt_every=60, plot_every=70)#, max_epochs, ckpt_every=60, plot_every=70)\n",
    "            helper.model.save(f\"{tf_dir}/models/HeLa/model_fold{i + 1}.keras\")\n",
    "        else: \n",
    "            helper.model.load(f\"{tf_dir}/models/HeLa/model_fold{i + 1}.keras\")\n",
    "    # Out-of-fold results\n",
    "    pred = helper.model.predict(val_ds.map(lambda X, y: X))\n",
    "    oof_true = list(val_ds.map(lambda X, y: y).rebatch(validation_size).take(1))[0]\n",
    "    oof_dice.append(dice_loss(oof_true, pred).numpy().mean())\n",
    "    oof_IoU.append(IoU(oof_true, pred).numpy().mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average out-of-fold IoU: {:.6f}\".format(np.mean(oof_IoU)))\n",
    "print(\"Average out-of-fold dice loss: {:.6f}\".format(np.mean(oof_dice)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the entire dataset\n",
    "\n",
    "Same as before, but this time for the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del helper\n",
    "except:\n",
    "    pass\n",
    "\n",
    "reset_seed()\n",
    "K.clear_session()\n",
    "\n",
    "train_ds = hela_train[\"01\"].concatenate(hela_train[\"02\"])\n",
    "training_size = train_ds.cardinality()\n",
    "train_ds = train_ds.shuffle(training_size, reshuffle_each_iteration=True)\\\n",
    "                   .repeat(2 * np.lcm(batch_size, training_size) // (training_size))\\\n",
    "                   .batch(batch_size, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                   .map(pipeline, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                   .prefetch(tf.data.AUTOTUNE)\n",
    "dist_train = strategy.experimental_distribute_dataset(train_ds)\n",
    "\n",
    "gc.collect()\n",
    "with strategy.scope():    \n",
    "    gc.collect()\n",
    "    helper = UNetHelper(strategy=strategy,\n",
    "                        model_param=model_param,\n",
    "                        loss_func=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                        optimizer=tf.keras.optimizers.SGD(learning_rate=max_lr, momentum=0.99),\n",
    "                        #opt_schedule=tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[5,], values=[1e-2, 1e-3]),\n",
    "                        )\n",
    "    if train_model:\n",
    "        train(helper, dist_train, None, None, max_epochs, ckpt_every=60, plot_every=None, verbose=True)\n",
    "        helper.model.save(f\"{tf_dir}/models/model_all.keras\")\n",
    "    else: \n",
    "        helper.model.load(f\"{tf_dir}/models/model_all.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(img):\n",
    "    clh = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clh_img = clh.apply(np.squeeze(img.numpy()))\n",
    "    return tf.constant(np.expand_dims(clh_img / 255.0, -1), dtype=tf.float32, shape=img.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hela_test = tfds.load(\"hela_test\", data_dir=tf_dir)\n",
    "\n",
    "hela_sub = hela_test[\"01\"].concatenate(hela_test[\"02\"])\\\n",
    "                          .map(lambda pair: tf.py_function(process_test, inp=[pair[\"image\"]], \n",
    "                                                           Tout=[tf.float32]),  \n",
    "                               num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                          .map(lambda X: tf.ensure_shape(X, img_shape))\\\n",
    "                          .cache(f\"{tf_dir}/TFCache/SUBMISSION\")\n",
    "hela_sub = hela_sub.batch(hela_sub.cardinality().numpy(), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = helper.model.predict(hela_sub.rebatch(2 * batch_size)).argmax(axis=-1)\n",
    "\n",
    "for i in range(sub_pred.shape[0]):\n",
    "    cv.imwrite(f\"Predictions/pred{str(i).zfill(4)}.png\", (sub_pred[i] * 255.0).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = list(hela_sub.take(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 7\n",
    "\n",
    "subtitles = [\"Image\", \"Predicted Mask\"]\n",
    "image_list = [X_t[j], sub_pred[j]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 24))\n",
    "for i in range(2):\n",
    "    ax[i].imshow(image_list[i], cmap=\"gray\")\n",
    "    ax[i].set_title(subtitles[i])              \n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=dQw4w9WgXcQ"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1108926,
     "sourceId": 5506221,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pdrop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
